{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyhQamEt9QCR"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "n8EH88or9SX6",
        "outputId": "05151fae-c55d-4f23-e345-59cc33cdd947"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "#Download your api kaggle.jdon from kaggle and upload it here\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntoZjYOb9dFA",
        "outputId": "a319ea46-999f-4530-bc95-002c99a8a787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6uiKWzw9iyP",
        "outputId": "33746854-0b50-4454-d2be-dd47c278ef71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/tawsifurrahman/tuberculosis-tb-chest-xray-dataset\n",
            "License(s): copyright-authors\n",
            "Downloading tuberculosis-tb-chest-xray-dataset.zip to /content\n",
            "100% 662M/663M [00:32<00:00, 22.8MB/s]\n",
            "100% 663M/663M [00:32<00:00, 21.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d tawsifurrahman/tuberculosis-tb-chest-xray-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUGp6PAc90fN",
        "outputId": "d4cbea51-f949-4289-e93e-37ee248d9363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "#unziping the file\n",
        "from zipfile import ZipFile\n",
        "file_name = '/content/tuberculosis-tb-chest-xray-dataset.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p-sBZW0-Aeu",
        "outputId": "373c7677-3008-449d-c4e8-db0d7eac432b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "source": [
        "! pip install split-folders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG-dgIQt-FzS",
        "outputId": "894c4a71-81c7-408b-a210-935595128a35"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define paths\n",
        "data_dir = '/content/TB_Chest_Radiography_Database'\n",
        "normal_dir = os.path.join(data_dir, 'Normal')\n",
        "tb_dir = os.path.join(data_dir, 'Tuberculosis')\n",
        "\n",
        "# Load images and labels\n",
        "normal_images = [os.path.join(normal_dir, img) for img in os.listdir(normal_dir)]\n",
        "tb_images = [os.path.join(tb_dir, img) for img in os.listdir(tb_dir)]\n",
        "images = normal_images + tb_images\n",
        "labels = [0] * len(normal_images) + [1] * len(tb_images)  # 0 for normal, 1 for tuberculosis\n",
        "\n",
        "# Resize and normalize images\n",
        "def preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (224, 224))  # Resize to common size\n",
        "    img = img.astype(np.float32) / 255.0  # Normalize pixel values\n",
        "    return img\n",
        "\n",
        "# Preprocess all images\n",
        "processed_images = [preprocess_image(img) for img in images]\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(processed_images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to TensorFlow tensors\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "# Shuffle and batch the datasets\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_images)).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# Count the number of samples in each dataset\n",
        "train_count = len(train_images)\n",
        "test_count = len(test_images)\n",
        "\n",
        "# Print the number of samples in each dataset\n",
        "print(\"Number of samples in the Train Dataset:\", train_count)\n",
        "print(\"Number of samples in the Test Dataset:\", test_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW0gsmPzB2JD",
        "outputId": "e4e98e5f-cdb5-488e-f2a1-bcbd9622de65"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define DenseNet model architecture\n",
        "def create_DenseNet(input_shape, num_classes, learning_rate=0.001, dropout_rate=0.5):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 2  # 2 classes: Normal and Tuberculosis\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "learning_rates = [0.001, 0.0001]\n",
        "dropout_rates = [0.5, 0.7]\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "for learning_rate in learning_rates:\n",
        "    for dropout_rate in dropout_rates:\n",
        "        print(f\"Training model with learning rate: {learning_rate}, dropout rate: {dropout_rate}\")\n",
        "\n",
        "        # Create the DenseNet model\n",
        "        model = create_DenseNet(input_shape, num_classes, learning_rate, dropout_rate)\n",
        "\n",
        "        # Define callbacks\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "        model_checkpoint = ModelCheckpoint(filepath='densenet_best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(train_dataset, epochs=epochs, batch_size=batch_size,\n",
        "                            validation_data=test_dataset, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "        # Evaluate the model\n",
        "        _, test_accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "        print(f\"Test accuracy for current configuration: {test_accuracy}\")\n",
        "\n",
        "        # Check if current model is the best so far\n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_model = model\n",
        "\n",
        "print(\"Hyperparameter tuning completed.\")\n",
        "print(f\"Best test accuracy: {best_accuracy}\")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save('best_densenet_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "EOvVdXGJ5Pay",
        "outputId": "7b4c746a-83e0-4adb-dc47-481d02c65ba6"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the best model\n",
        "best_model = load_model('best_densenet_model.h5')\n",
        "\n",
        "# Predict on the test dataset\n",
        "y_pred = np.argmax(best_model.predict(test_dataset), axis=-1)\n",
        "\n",
        "# Get true labels\n",
        "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = np.trace(cm) / np.sum(cm)\n",
        "precision = cm[1, 1] / np.sum(cm[:, 1])\n",
        "recall = cm[1, 1] / np.sum(cm[1, :])\n",
        "sensitivity = recall\n",
        "\n",
        "# Print the performance metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "\n",
        "# Generate a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Tuberculosis\"], yticklabels=[\"Normal\", \"Tuberculosis\"])\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"Tuberculosis\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeXZk1gEfNGM",
        "outputId": "4d15890e-bdfc-42d2-e828-30ac1f7c9ffc"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define paths\n",
        "data_dir = '/content/TB_Chest_Radiography_Database'\n",
        "normal_dir = os.path.join(data_dir, 'Normal')\n",
        "tb_dir = os.path.join(data_dir, 'Tuberculosis')\n",
        "\n",
        "# Load images and labels\n",
        "normal_images = [os.path.join(normal_dir, img) for img in os.listdir(normal_dir)]\n",
        "tb_images = [os.path.join(tb_dir, img) for img in os.listdir(tb_dir)]\n",
        "images = normal_images + tb_images\n",
        "labels = [0] * len(normal_images) + [1] * len(tb_images)  # 0 for normal, 1 for tuberculosis\n",
        "\n",
        "# Resize and normalize images\n",
        "def preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (224, 224))  # Resize to common size\n",
        "    img = img.astype(np.float32) / 255.0  # Normalize pixel values\n",
        "    return img\n",
        "\n",
        "# Preprocess all images\n",
        "processed_images = [preprocess_image(img) for img in images]\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(processed_images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to TensorFlow tensors\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "# Shuffle and batch the datasets\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_images)).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# Define DenseNet model architecture\n",
        "def create_DenseNet(input_shape, num_classes, learning_rate=0.001, dropout_rate=0.5):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 2  # 2 classes: Normal and Tuberculosis\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "learning_rates = [0.001, 0.0001]\n",
        "dropout_rates = [0.5, 0.7]\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "early_stopping_patience = 5  # Adjusted early stopping patience\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "for learning_rate in learning_rates:\n",
        "    for dropout_rate in dropout_rates:\n",
        "        print(f\"Training model with learning rate: {learning_rate}, dropout rate: {dropout_rate}\")\n",
        "\n",
        "        # Create the DenseNet model\n",
        "        model = create_DenseNet(input_shape, num_classes, learning_rate, dropout_rate)\n",
        "\n",
        "        # Define callbacks\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True)\n",
        "        model_checkpoint = ModelCheckpoint(filepath='densenet_best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(train_dataset, epochs=epochs, batch_size=batch_size,\n",
        "                            validation_data=test_dataset, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "        # Evaluate the model\n",
        "        _, test_accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "        print(f\"Test accuracy for current configuration: {test_accuracy}\")\n",
        "\n",
        "        # Check if current model is the best so far\n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_model = model\n",
        "\n",
        "print(\"Hyperparameter tuning completed.\")\n",
        "print(f\"Best test accuracy: {best_accuracy}\")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save('best_densenet_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "XA7jm2WFOMm1",
        "outputId": "1e8f51cc-f747-4802-c3d6-592f460f75d6"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the best model\n",
        "best_model = load_model('best_densenet_model.h5')\n",
        "\n",
        "# Predict on the test dataset\n",
        "y_pred = np.argmax(best_model.predict(test_dataset), axis=-1)\n",
        "\n",
        "# Get true labels\n",
        "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = np.trace(cm) / np.sum(cm)\n",
        "precision = cm[1, 1] / np.sum(cm[:, 1])\n",
        "recall = cm[1, 1] / np.sum(cm[1, :])\n",
        "sensitivity = recall\n",
        "\n",
        "# Print the performance metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "\n",
        "# Generate a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Tuberculosis\"], yticklabels=[\"Normal\", \"Tuberculosis\"])\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"Tuberculosis\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKl8wA6jpchQ",
        "outputId": "3a413ccd-9a50-40c7-e2da-e267a7ad625f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with learning rate: 0.001, dropout rate: 0.5\n",
            "Epoch 1/10\n",
            "105/105 [==============================] - 19s 93ms/step - loss: 0.7018 - accuracy: 0.8780 - val_loss: 4.0301 - val_accuracy: 0.1893\n",
            "Epoch 2/10\n",
            "  1/105 [..............................] - ETA: 8s - loss: 0.1065 - accuracy: 0.9688"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "105/105 [==============================] - 9s 84ms/step - loss: 0.5183 - accuracy: 0.9342 - val_loss: 0.7898 - val_accuracy: 0.9238\n",
            "Epoch 3/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 0.6036 - accuracy: 0.9438 - val_loss: 1.6777 - val_accuracy: 0.9048\n",
            "Epoch 4/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 0.4562 - accuracy: 0.9539 - val_loss: 4.8152 - val_accuracy: 0.8607\n",
            "Epoch 5/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 0.5093 - accuracy: 0.9571 - val_loss: 0.7118 - val_accuracy: 0.9536\n",
            "Epoch 6/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 0.4179 - accuracy: 0.9649 - val_loss: 2.8474 - val_accuracy: 0.8262\n",
            "Epoch 7/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.4785 - accuracy: 0.9589 - val_loss: 5.4384 - val_accuracy: 0.8702\n",
            "Epoch 8/10\n",
            "105/105 [==============================] - 9s 82ms/step - loss: 0.3144 - accuracy: 0.9702 - val_loss: 4.1315 - val_accuracy: 0.8905\n",
            "Epoch 9/10\n",
            "105/105 [==============================] - 9s 82ms/step - loss: 0.3622 - accuracy: 0.9679 - val_loss: 5.6336 - val_accuracy: 0.8488\n",
            "Epoch 10/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 0.3493 - accuracy: 0.9693 - val_loss: 0.5991 - val_accuracy: 0.9631\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.5991 - accuracy: 0.9631\n",
            "Test accuracy for current configuration: 0.9630952477455139\n",
            "Training model with learning rate: 0.001, dropout rate: 0.7\n",
            "Epoch 1/10\n",
            "105/105 [==============================] - 14s 85ms/step - loss: 1.2782 - accuracy: 0.8307 - val_loss: 0.5125 - val_accuracy: 0.8107\n",
            "Epoch 2/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 1.1708 - accuracy: 0.8890 - val_loss: 0.9224 - val_accuracy: 0.8833\n",
            "Epoch 3/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.9878 - accuracy: 0.9161 - val_loss: 12.3194 - val_accuracy: 0.3905\n",
            "Epoch 4/10\n",
            "105/105 [==============================] - 9s 88ms/step - loss: 0.9202 - accuracy: 0.9321 - val_loss: 12.1802 - val_accuracy: 0.5286\n",
            "Epoch 5/10\n",
            "105/105 [==============================] - 9s 82ms/step - loss: 0.7647 - accuracy: 0.9402 - val_loss: 5.3510 - val_accuracy: 0.7464\n",
            "Epoch 6/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.7476 - accuracy: 0.9449 - val_loss: 27.9465 - val_accuracy: 0.3060\n",
            "27/27 [==============================] - 1s 24ms/step - loss: 0.5125 - accuracy: 0.8107\n",
            "Test accuracy for current configuration: 0.8107143044471741\n",
            "Training model with learning rate: 0.0001, dropout rate: 0.5\n",
            "Epoch 1/10\n",
            "105/105 [==============================] - 13s 86ms/step - loss: 0.4633 - accuracy: 0.8506 - val_loss: 0.9528 - val_accuracy: 0.1631\n",
            "Epoch 2/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.2624 - accuracy: 0.9182 - val_loss: 2.5957 - val_accuracy: 0.1631\n",
            "Epoch 3/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 0.1799 - accuracy: 0.9432 - val_loss: 4.4741 - val_accuracy: 0.1631\n",
            "Epoch 4/10\n",
            "105/105 [==============================] - 9s 89ms/step - loss: 0.1474 - accuracy: 0.9592 - val_loss: 3.7798 - val_accuracy: 0.1631\n",
            "Epoch 5/10\n",
            "105/105 [==============================] - 9s 82ms/step - loss: 0.1506 - accuracy: 0.9527 - val_loss: 1.9555 - val_accuracy: 0.3071\n",
            "Epoch 6/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.1645 - accuracy: 0.9542 - val_loss: 0.4575 - val_accuracy: 0.8381\n",
            "Epoch 7/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 0.1261 - accuracy: 0.9643 - val_loss: 0.3087 - val_accuracy: 0.9143\n",
            "Epoch 8/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.1125 - accuracy: 0.9693 - val_loss: 0.2681 - val_accuracy: 0.9060\n",
            "Epoch 9/10\n",
            "105/105 [==============================] - 9s 82ms/step - loss: 0.1173 - accuracy: 0.9649 - val_loss: 0.9372 - val_accuracy: 0.7702\n",
            "Epoch 10/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.1012 - accuracy: 0.9717 - val_loss: 0.6594 - val_accuracy: 0.9119\n",
            "27/27 [==============================] - 1s 24ms/step - loss: 0.6594 - accuracy: 0.9119\n",
            "Test accuracy for current configuration: 0.9119047522544861\n",
            "Training model with learning rate: 0.0001, dropout rate: 0.7\n",
            "Epoch 1/10\n",
            "105/105 [==============================] - 13s 87ms/step - loss: 0.5876 - accuracy: 0.8330 - val_loss: 2.9311 - val_accuracy: 0.1631\n",
            "Epoch 2/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 0.4907 - accuracy: 0.8649 - val_loss: 4.1449 - val_accuracy: 0.1631\n",
            "Epoch 3/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.4451 - accuracy: 0.8872 - val_loss: 2.9159 - val_accuracy: 0.1631\n",
            "Epoch 4/10\n",
            "105/105 [==============================] - 9s 89ms/step - loss: 0.3969 - accuracy: 0.8976 - val_loss: 1.5232 - val_accuracy: 0.1631\n",
            "Epoch 5/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.3408 - accuracy: 0.9086 - val_loss: 0.4901 - val_accuracy: 0.8476\n",
            "Epoch 6/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.3831 - accuracy: 0.9021 - val_loss: 3.3660 - val_accuracy: 0.1631\n",
            "Epoch 7/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.3727 - accuracy: 0.9101 - val_loss: 1.5483 - val_accuracy: 0.1655\n",
            "Epoch 8/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 0.3427 - accuracy: 0.9217 - val_loss: 0.3571 - val_accuracy: 0.9083\n",
            "Epoch 9/10\n",
            "105/105 [==============================] - 9s 83ms/step - loss: 0.3296 - accuracy: 0.9220 - val_loss: 2.4557 - val_accuracy: 0.1631\n",
            "Epoch 10/10\n",
            "105/105 [==============================] - 9s 84ms/step - loss: 0.3432 - accuracy: 0.9158 - val_loss: 0.5699 - val_accuracy: 0.8369\n",
            "27/27 [==============================] - 1s 24ms/step - loss: 0.5699 - accuracy: 0.8369\n",
            "Test accuracy for current configuration: 0.836904764175415\n",
            "Hyperparameter tuning completed.\n",
            "Best test accuracy: 0.9630952477455139\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define paths\n",
        "data_dir = '/content/TB_Chest_Radiography_Database'\n",
        "normal_dir = os.path.join(data_dir, 'Normal')\n",
        "tb_dir = os.path.join(data_dir, 'Tuberculosis')\n",
        "\n",
        "# Load images and labels\n",
        "normal_images = [os.path.join(normal_dir, img) for img in os.listdir(normal_dir)]\n",
        "tb_images = [os.path.join(tb_dir, img) for img in os.listdir(tb_dir)]\n",
        "images = normal_images + tb_images\n",
        "labels = [0] * len(normal_images) + [1] * len(tb_images)  # 0 for normal, 1 for tuberculosis\n",
        "\n",
        "# Resize and normalize images\n",
        "def preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (224, 224))  # Resize to common size\n",
        "    img = img.astype(np.float32) / 255.0  # Normalize pixel values\n",
        "    return img\n",
        "\n",
        "# Preprocess all images\n",
        "processed_images = [preprocess_image(img) for img in images]\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(processed_images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to TensorFlow tensors\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "# Shuffle and batch the datasets\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_images)).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# Define DenseNet model architecture\n",
        "def create_DenseNet(input_shape, num_classes, learning_rate=0.001, dropout_rate=0.5):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Conv2D(128, (1, 1), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 2  # 2 classes: Normal and Tuberculosis\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "learning_rates = [0.001, 0.0001]\n",
        "dropout_rates = [0.5, 0.7]\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "early_stopping_patience = 5  # Adjusted early stopping patience\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "for learning_rate in learning_rates:\n",
        "    for dropout_rate in dropout_rates:\n",
        "        print(f\"Training model with learning rate: {learning_rate}, dropout rate: {dropout_rate}\")\n",
        "\n",
        "        # Create the DenseNet model\n",
        "        model = create_DenseNet(input_shape, num_classes, learning_rate, dropout_rate)\n",
        "\n",
        "        # Define callbacks\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True)\n",
        "        model_checkpoint = ModelCheckpoint(filepath='densenet_best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(train_dataset, epochs=epochs, batch_size=batch_size,\n",
        "                            validation_data=test_dataset, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "        # Evaluate the model\n",
        "        _, test_accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "        print(f\"Test accuracy for current configuration: {test_accuracy}\")\n",
        "\n",
        "        # Check if current model is the best so far\n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_model = model\n",
        "\n",
        "print(\"Hyperparameter tuning completed.\")\n",
        "print(f\"Best test accuracy: {best_accuracy}\")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save('best_densenet_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "ZvazesAhwQoH",
        "outputId": "848a9386-e852-41b4-9e7b-8743eb1e02b3"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the best model\n",
        "best_model = load_model('best_densenet_model.h5')\n",
        "\n",
        "# Predict on the test dataset\n",
        "y_pred = np.argmax(best_model.predict(test_dataset), axis=-1)\n",
        "\n",
        "# Get true labels\n",
        "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = np.trace(cm) / np.sum(cm)\n",
        "precision = cm[1, 1] / np.sum(cm[:, 1])\n",
        "recall = cm[1, 1] / np.sum(cm[1, :])\n",
        "sensitivity = recall\n",
        "\n",
        "# Print the performance metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "\n",
        "# Generate a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Tuberculosis\"], yticklabels=[\"Normal\", \"Tuberculosis\"])\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"Tuberculosis\"]))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
